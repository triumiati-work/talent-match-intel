{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f500e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import altair as alt\n",
    "\n",
    "# Environment and database\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d30eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Supabase\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "load_dotenv(dotenv_path=\"D:/Work/Projects/Notebooks/talent-match-intel/streamlit_app/key.env\")\n",
    "\n",
    "# read without printing secrets\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "print(\"‚úÖ Connected to Supabase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546f7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(table_name, chunk_size=1000):\n",
    "    all_data = []\n",
    "    offset = 0\n",
    "    \n",
    "    print(f\"üîÑ Starting unlimited load for: {table_name}\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Fetch data using .range() starting from the current offset\n",
    "            response = (\n",
    "                supabase.table(table_name)\n",
    "                .select(\"*\")\n",
    "                .range(offset, offset + chunk_size - 1)\n",
    "                .execute()\n",
    "            )\n",
    "            \n",
    "            data = response.data\n",
    "            \n",
    "            # If the response is empty, we've loaded all records\n",
    "            if not data:\n",
    "                break\n",
    "                \n",
    "            # Add the fetched chunk of data to the master list\n",
    "            all_data.extend(data)\n",
    "            \n",
    "            # Increment the offset to prepare for the next chunk\n",
    "            offset += chunk_size\n",
    "            \n",
    "            print(f\"   -> Fetched {len(data)} records. Total: {len(all_data)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {table_name} at offset {offset}: {e}\")\n",
    "            break # Exit the loop on error\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    print(f\"‚úÖ Loaded {table_name}: {len(df)} records in total.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832ee8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting unlimited load for: employees\n",
      "   -> Fetched 1000 records. Total: 1000\n",
      "   -> Fetched 1000 records. Total: 2000\n",
      "   -> Fetched 10 records. Total: 2010\n",
      "‚úÖ Loaded employees: 2010 records in total.\n",
      "üîÑ Starting unlimited load for: performance_yearly\n",
      "   -> Fetched 1000 records. Total: 1000\n",
      "   -> Fetched 1000 records. Total: 2000\n",
      "   -> Fetched 1000 records. Total: 3000\n",
      "   -> Fetched 1000 records. Total: 4000\n",
      "   -> Fetched 1000 records. Total: 5000\n",
      "   -> Fetched 1000 records. Total: 6000\n",
      "   -> Fetched 1000 records. Total: 7000\n",
      "   -> Fetched 1000 records. Total: 8000\n",
      "   -> Fetched 1000 records. Total: 9000\n",
      "   -> Fetched 1000 records. Total: 10000\n",
      "   -> Fetched 50 records. Total: 10050\n",
      "‚úÖ Loaded performance_yearly: 10050 records in total.\n",
      "üîÑ Starting unlimited load for: competencies_yearly\n",
      "   -> Fetched 1000 records. Total: 1000\n",
      "   -> Fetched 1000 records. Total: 2000\n",
      "   -> Fetched 1000 records. Total: 3000\n",
      "   -> Fetched 1000 records. Total: 4000\n",
      "   -> Fetched 1000 records. Total: 5000\n",
      "   -> Fetched 1000 records. Total: 6000\n",
      "   -> Fetched 1000 records. Total: 7000\n",
      "   -> Fetched 1000 records. Total: 8000\n",
      "   -> Fetched 1000 records. Total: 9000\n",
      "   -> Fetched 1000 records. Total: 10000\n",
      "   -> Fetched 1000 records. Total: 11000\n",
      "   -> Fetched 1000 records. Total: 12000\n",
      "   -> Fetched 1000 records. Total: 13000\n",
      "   -> Fetched 1000 records. Total: 14000\n",
      "   -> Fetched 1000 records. Total: 15000\n",
      "   -> Fetched 1000 records. Total: 16000\n",
      "   -> Fetched 1000 records. Total: 17000\n",
      "   -> Fetched 1000 records. Total: 18000\n",
      "   -> Fetched 1000 records. Total: 19000\n",
      "   -> Fetched 1000 records. Total: 20000\n",
      "   -> Fetched 1000 records. Total: 21000\n",
      "   -> Fetched 1000 records. Total: 22000\n",
      "   -> Fetched 1000 records. Total: 23000\n",
      "   -> Fetched 1000 records. Total: 24000\n",
      "   -> Fetched 1000 records. Total: 25000\n",
      "   -> Fetched 1000 records. Total: 26000\n",
      "   -> Fetched 1000 records. Total: 27000\n",
      "   -> Fetched 1000 records. Total: 28000\n",
      "   -> Fetched 1000 records. Total: 29000\n",
      "   -> Fetched 1000 records. Total: 30000\n",
      "   -> Fetched 1000 records. Total: 31000\n",
      "   -> Fetched 1000 records. Total: 32000\n",
      "   -> Fetched 1000 records. Total: 33000\n",
      "   -> Fetched 1000 records. Total: 34000\n",
      "   -> Fetched 1000 records. Total: 35000\n",
      "   -> Fetched 1000 records. Total: 36000\n",
      "   -> Fetched 1000 records. Total: 37000\n",
      "   -> Fetched 1000 records. Total: 38000\n",
      "   -> Fetched 1000 records. Total: 39000\n",
      "   -> Fetched 1000 records. Total: 40000\n",
      "   -> Fetched 1000 records. Total: 41000\n",
      "   -> Fetched 1000 records. Total: 42000\n",
      "   -> Fetched 1000 records. Total: 43000\n",
      "   -> Fetched 1000 records. Total: 44000\n",
      "   -> Fetched 1000 records. Total: 45000\n",
      "   -> Fetched 1000 records. Total: 46000\n",
      "   -> Fetched 1000 records. Total: 47000\n",
      "   -> Fetched 1000 records. Total: 48000\n",
      "   -> Fetched 1000 records. Total: 49000\n",
      "   -> Fetched 1000 records. Total: 50000\n",
      "   -> Fetched 1000 records. Total: 51000\n",
      "   -> Fetched 1000 records. Total: 52000\n",
      "   -> Fetched 1000 records. Total: 53000\n",
      "   -> Fetched 1000 records. Total: 54000\n",
      "   -> Fetched 1000 records. Total: 55000\n",
      "   -> Fetched 1000 records. Total: 56000\n",
      "   -> Fetched 1000 records. Total: 57000\n",
      "   -> Fetched 1000 records. Total: 58000\n",
      "   -> Fetched 1000 records. Total: 59000\n",
      "   -> Fetched 1000 records. Total: 60000\n",
      "   -> Fetched 1000 records. Total: 61000\n",
      "   -> Fetched 1000 records. Total: 62000\n",
      "   -> Fetched 1000 records. Total: 63000\n",
      "   -> Fetched 1000 records. Total: 64000\n",
      "   -> Fetched 1000 records. Total: 65000\n",
      "   -> Fetched 1000 records. Total: 66000\n",
      "   -> Fetched 1000 records. Total: 67000\n",
      "   -> Fetched 1000 records. Total: 68000\n",
      "   -> Fetched 1000 records. Total: 69000\n",
      "   -> Fetched 1000 records. Total: 70000\n",
      "   -> Fetched 1000 records. Total: 71000\n",
      "   -> Fetched 1000 records. Total: 72000\n",
      "   -> Fetched 1000 records. Total: 73000\n",
      "   -> Fetched 1000 records. Total: 74000\n",
      "   -> Fetched 1000 records. Total: 75000\n",
      "   -> Fetched 1000 records. Total: 76000\n",
      "   -> Fetched 1000 records. Total: 77000\n",
      "   -> Fetched 1000 records. Total: 78000\n",
      "   -> Fetched 1000 records. Total: 79000\n",
      "   -> Fetched 1000 records. Total: 80000\n",
      "   -> Fetched 1000 records. Total: 81000\n",
      "   -> Fetched 1000 records. Total: 82000\n",
      "   -> Fetched 1000 records. Total: 83000\n",
      "   -> Fetched 1000 records. Total: 84000\n",
      "   -> Fetched 1000 records. Total: 85000\n",
      "   -> Fetched 1000 records. Total: 86000\n",
      "   -> Fetched 1000 records. Total: 87000\n",
      "   -> Fetched 1000 records. Total: 88000\n",
      "   -> Fetched 1000 records. Total: 89000\n",
      "   -> Fetched 1000 records. Total: 90000\n",
      "   -> Fetched 1000 records. Total: 91000\n",
      "   -> Fetched 1000 records. Total: 92000\n",
      "   -> Fetched 1000 records. Total: 93000\n",
      "   -> Fetched 1000 records. Total: 94000\n",
      "   -> Fetched 1000 records. Total: 95000\n",
      "   -> Fetched 1000 records. Total: 96000\n",
      "   -> Fetched 1000 records. Total: 97000\n",
      "   -> Fetched 1000 records. Total: 98000\n",
      "   -> Fetched 1000 records. Total: 99000\n",
      "   -> Fetched 1000 records. Total: 100000\n",
      "   -> Fetched 500 records. Total: 100500\n",
      "‚úÖ Loaded competencies_yearly: 100500 records in total.\n",
      "üîÑ Starting unlimited load for: profiles_psych\n",
      "   -> Fetched 1000 records. Total: 1000\n",
      "   -> Fetched 1000 records. Total: 2000\n",
      "   -> Fetched 10 records. Total: 2010\n",
      "‚úÖ Loaded profiles_psych: 2010 records in total.\n",
      "üîÑ Starting unlimited load for: papi_scores\n",
      "   -> Fetched 1000 records. Total: 1000\n",
      "   -> Fetched 1000 records. Total: 2000\n",
      "   -> Fetched 1000 records. Total: 3000\n",
      "   -> Fetched 1000 records. Total: 4000\n",
      "   -> Fetched 1000 records. Total: 5000\n",
      "   -> Fetched 1000 records. Total: 6000\n",
      "   -> Fetched 1000 records. Total: 7000\n",
      "   -> Fetched 1000 records. Total: 8000\n",
      "   -> Fetched 1000 records. Total: 9000\n",
      "   -> Fetched 1000 records. Total: 10000\n",
      "   -> Fetched 1000 records. Total: 11000\n",
      "   -> Fetched 1000 records. Total: 12000\n",
      "   -> Fetched 1000 records. Total: 13000\n",
      "   -> Fetched 1000 records. Total: 14000\n",
      "   -> Fetched 1000 records. Total: 15000\n",
      "   -> Fetched 1000 records. Total: 16000\n",
      "   -> Fetched 1000 records. Total: 17000\n",
      "   -> Fetched 1000 records. Total: 18000\n",
      "   -> Fetched 1000 records. Total: 19000\n",
      "   -> Fetched 1000 records. Total: 20000\n",
      "   -> Fetched 1000 records. Total: 21000\n",
      "   -> Fetched 1000 records. Total: 22000\n",
      "   -> Fetched 1000 records. Total: 23000\n",
      "   -> Fetched 1000 records. Total: 24000\n",
      "   -> Fetched 1000 records. Total: 25000\n",
      "   -> Fetched 1000 records. Total: 26000\n",
      "   -> Fetched 1000 records. Total: 27000\n",
      "   -> Fetched 1000 records. Total: 28000\n",
      "   -> Fetched 1000 records. Total: 29000\n",
      "   -> Fetched 1000 records. Total: 30000\n",
      "   -> Fetched 1000 records. Total: 31000\n",
      "   -> Fetched 1000 records. Total: 32000\n",
      "   -> Fetched 1000 records. Total: 33000\n",
      "   -> Fetched 1000 records. Total: 34000\n",
      "   -> Fetched 1000 records. Total: 35000\n",
      "   -> Fetched 1000 records. Total: 36000\n",
      "   -> Fetched 1000 records. Total: 37000\n",
      "   -> Fetched 1000 records. Total: 38000\n",
      "   -> Fetched 1000 records. Total: 39000\n",
      "   -> Fetched 1000 records. Total: 40000\n",
      "   -> Fetched 200 records. Total: 40200\n",
      "‚úÖ Loaded papi_scores: 40200 records in total.\n",
      "üîÑ Starting unlimited load for: strengths\n",
      "   -> Fetched 1000 records. Total: 1000\n",
      "   -> Fetched 1000 records. Total: 2000\n",
      "   -> Fetched 1000 records. Total: 3000\n",
      "   -> Fetched 1000 records. Total: 4000\n",
      "   -> Fetched 1000 records. Total: 5000\n",
      "   -> Fetched 1000 records. Total: 6000\n",
      "   -> Fetched 1000 records. Total: 7000\n",
      "   -> Fetched 1000 records. Total: 8000\n",
      "   -> Fetched 1000 records. Total: 9000\n",
      "   -> Fetched 1000 records. Total: 10000\n",
      "   -> Fetched 1000 records. Total: 11000\n",
      "   -> Fetched 1000 records. Total: 12000\n",
      "   -> Fetched 1000 records. Total: 13000\n",
      "   -> Fetched 1000 records. Total: 14000\n",
      "   -> Fetched 1000 records. Total: 15000\n",
      "   -> Fetched 1000 records. Total: 16000\n",
      "   -> Fetched 1000 records. Total: 17000\n",
      "   -> Fetched 1000 records. Total: 18000\n",
      "   -> Fetched 1000 records. Total: 19000\n",
      "   -> Fetched 1000 records. Total: 20000\n",
      "   -> Fetched 1000 records. Total: 21000\n",
      "   -> Fetched 1000 records. Total: 22000\n",
      "   -> Fetched 1000 records. Total: 23000\n",
      "   -> Fetched 1000 records. Total: 24000\n",
      "   -> Fetched 1000 records. Total: 25000\n",
      "   -> Fetched 1000 records. Total: 26000\n",
      "   -> Fetched 1000 records. Total: 27000\n",
      "   -> Fetched 1000 records. Total: 28000\n",
      "   -> Fetched 140 records. Total: 28140\n",
      "‚úÖ Loaded strengths: 28140 records in total.\n",
      "üîÑ Starting unlimited load for: dim_competency_pillars\n",
      "   -> Fetched 10 records. Total: 10\n",
      "‚úÖ Loaded dim_competency_pillars: 10 records in total.\n",
      "üîÑ Starting unlimited load for: dim_grades\n",
      "   -> Fetched 3 records. Total: 3\n",
      "‚úÖ Loaded dim_grades: 3 records in total.\n",
      "üîÑ Starting unlimited load for: dim_positions\n",
      "   -> Fetched 6 records. Total: 6\n",
      "‚úÖ Loaded dim_positions: 6 records in total.\n",
      "üîÑ Starting unlimited load for: dim_education\n",
      "   -> Fetched 4 records. Total: 4\n",
      "‚úÖ Loaded dim_education: 4 records in total.\n",
      "üîÑ Starting unlimited load for: dim_departments\n",
      "   -> Fetched 6 records. Total: 6\n",
      "‚úÖ Loaded dim_departments: 6 records in total.\n",
      "üîÑ Starting unlimited load for: dim_areas\n",
      "   -> Fetched 4 records. Total: 4\n",
      "‚úÖ Loaded dim_areas: 4 records in total.\n",
      "üîÑ Starting unlimited load for: dim_divisions\n",
      "   -> Fetched 5 records. Total: 5\n",
      "‚úÖ Loaded dim_divisions: 5 records in total.\n",
      "üîÑ Starting unlimited load for: dim_directorates\n",
      "   -> Fetched 3 records. Total: 3\n",
      "‚úÖ Loaded dim_directorates: 3 records in total.\n",
      "üîÑ Starting unlimited load for: dim_majors\n",
      "   -> Fetched 6 records. Total: 6\n",
      "‚úÖ Loaded dim_majors: 6 records in total.\n"
     ]
    }
   ],
   "source": [
    "# Load all tables\n",
    "df_employees = load_table('employees')\n",
    "df_performance = load_table('performance_yearly')\n",
    "df_competencies = load_table('competencies_yearly')\n",
    "df_psych = load_table('profiles_psych')\n",
    "df_papi = load_table('papi_scores')\n",
    "df_strengths = load_table('strengths')\n",
    "df_pillars = load_table('dim_competency_pillars')\n",
    "df_grades = load_table('dim_grades')\n",
    "df_positions = load_table('dim_positions')\n",
    "df_education = load_table('dim_education')\n",
    "df_departments = load_table('dim_departments')\n",
    "df_areas = load_table('dim_areas')\n",
    "df_divisions = load_table('dim_divisions')\n",
    "df_directorates = load_table('dim_directorates')\n",
    "df_majors = load_table('dim_majors')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7eae9",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab613e",
   "metadata": {},
   "source": [
    "## Competency Pillars Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf620eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Merge Competency Scores with Competency Pillars/Labels\n",
    "df_comp_analysis = df_competencies.merge(\n",
    "    df_pillars[['pillar_code', 'pillar_label']],\n",
    "    on='pillar_code',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 2. Merge Competency Data with Final Employee Rating\n",
    "# Assuming 'employee_id' is the join key\n",
    "df_comp_analysis = df_comp_analysis.merge(\n",
    "    df_performance[['employee_id', 'rating']],\n",
    "    on='employee_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# 3. Aggregate Competency Scores\n",
    "# For a holistic view, group the competency scores by Pillar and Rating for T-test preparation.\n",
    "df_comp_analysis_ready = df_comp_analysis.dropna(subset=['pillar_label', 'rating', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1fa7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CRITICAL FIX: Convert the 'score' column to numeric, forcing errors to NaN.\n",
    "df_comp_analysis_ready['score'] = pd.to_numeric(df_comp_analysis_ready['score'], errors='coerce')\n",
    "\n",
    "\n",
    "# Define Cohen's d Function (for effect size)\n",
    "def calculate_cohens_d(group1, group2):\n",
    "    \"\"\"Calculates Cohen's d for two independent groups.\"\"\"\n",
    "    # ... (function body remains the same) ...\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    s1, s2 = group1.std(), group2.std()\n",
    "    \n",
    "    if n1 + n2 - 2 <= 0: return np.nan\n",
    "    sp = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
    "    if sp == 0: return np.nan\n",
    "        \n",
    "    d = (group1.mean() - group2.mean()) / sp\n",
    "    return d\n",
    "\n",
    "competency_test_results = []\n",
    "competency_list = df_comp_analysis_ready['pillar_label'].unique()\n",
    "MIN_SAMPLE_SIZE = 10 \n",
    "\n",
    "for pillar in competency_list:\n",
    "    # Separate the scores for High (5) and Low (1/2) Performers for the current pillar\n",
    "    # NOTE: .dropna() is still essential here to remove the NaN values created by pd.to_numeric\n",
    "    high_scores = df_comp_analysis_ready[\n",
    "        (df_comp_analysis_ready['pillar_label'] == pillar) & \n",
    "        (df_comp_analysis_ready['rating'] == 5.0)\n",
    "    ]['score'].dropna()\n",
    "    \n",
    "    low_scores = df_comp_analysis_ready[\n",
    "        (df_comp_analysis_ready['pillar_label'] == pillar) & \n",
    "        (df_comp_analysis_ready['rating'].isin([1.0, 2.0]))\n",
    "    ]['score'].dropna()\n",
    "    \n",
    "    # Proceed only if both groups have sufficient data\n",
    "    if len(high_scores) >= MIN_SAMPLE_SIZE and len(low_scores) >= MIN_SAMPLE_SIZE:\n",
    "        \n",
    "        # Welch's T-test (safer when variances are unequal)\n",
    "        # This line will now work because high_scores and low_scores are numeric Series\n",
    "        t_stat, p_value = stats.ttest_ind(high_scores, low_scores, equal_var=False) \n",
    "        \n",
    "        # Calculate Effect Size\n",
    "        cohens_d = calculate_cohens_d(high_scores, low_scores)\n",
    "        \n",
    "        competency_test_results.append({\n",
    "            'Competency_Pillar': pillar,\n",
    "            'Mean_R5': high_scores.mean(),\n",
    "            'Mean_R12': low_scores.mean(),\n",
    "            'Cohens_d': cohens_d,\n",
    "            'P_Value': p_value,\n",
    "            'Significant': p_value < 0.05 \n",
    "        })\n",
    "\n",
    "df_comp_results = pd.DataFrame(competency_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7945509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÜ Top Competency Differentiators (High vs. Low Performers) ---\n",
      "Ranked by Cohen's d (Largest Performance Gap)\n",
      "              Competency_Pillar  Mean_R5  Mean_R12  Cohens_d  P_Value Significant\n",
      "6      Value Creation for Users    3.588     2.990     0.160      0.0           ‚úÖ\n",
      "2  Insight & Decision Sharpness    3.536     2.968     0.156      0.0           ‚úÖ\n",
      "1   Curiosity & Experimentation    3.467     2.952     0.152      0.0           ‚úÖ\n",
      "5    Social Empathy & Awareness    3.672     3.027     0.150      0.0           ‚úÖ\n",
      "0     Growth Drive & Resilience    3.467     2.963     0.150      0.0           ‚úÖ\n",
      "3   Quality Delivery Discipline    3.768     3.051     0.141      0.0           ‚úÖ\n",
      "8    Forward Thinking & Clarity    3.592     3.016     0.136      0.0           ‚úÖ\n",
      "9     Commercial Savvy & Impact    3.495     3.009     0.126      0.0           ‚úÖ\n",
      "7       Lead, Inspire & Empower    3.672     3.085     0.119      0.0           ‚úÖ\n",
      "4    Synergy & Team Orientation    3.613     3.074     0.113      0.0           ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Filter for statistically significant results AND sort by Cohen's d\n",
    "df_final_ranking = (\n",
    "    df_comp_results\n",
    "    .sort_values('Cohens_d', ascending=False)\n",
    ")\n",
    "\n",
    "print(\"--- üèÜ Top Competency Differentiators (High vs. Low Performers) ---\")\n",
    "print(\"Ranked by Cohen's d (Largest Performance Gap)\")\n",
    "df_final_ranking['Significant'] = df_final_ranking['Significant'].apply(lambda x: '‚úÖ' if x else '‚ùå')\n",
    "print(df_final_ranking.round(3).head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f662036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-46e1055b6aec40059ead5a532649b163.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-46e1055b6aec40059ead5a532649b163.vega-embed details,\n",
       "  #altair-viz-46e1055b6aec40059ead5a532649b163.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-46e1055b6aec40059ead5a532649b163\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-46e1055b6aec40059ead5a532649b163\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-46e1055b6aec40059ead5a532649b163\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-374a71f4d4fc99749a3a3c1de3f01cc7\"}, \"mark\": {\"type\": \"bar\", \"color\": \"#1f77b4\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Competency_Pillar\", \"type\": \"nominal\"}, {\"field\": \"Cohens_d\", \"format\": \".3f\", \"type\": \"quantitative\"}, {\"field\": \"P_Value\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Cohens_d\", \"scale\": {\"domain\": [0.1, 0.17]}, \"title\": \"Performance Gap (Cohen's d)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Competency_Pillar\", \"sort\": \"-x\", \"title\": \"Competency Pillar\", \"type\": \"nominal\"}}, \"name\": \"view_1\", \"title\": \"\\ud83c\\udfc6 Top 10 Competency Differentiators (Ranked by Effect Size)\"}, {\"data\": {\"name\": \"data-f15c316850391f8b09c0452aaf6fbc5f\"}, \"mark\": {\"type\": \"bar\", \"opacity\": 0.8}, \"encoding\": {\"color\": {\"field\": \"Performer_Group\", \"title\": \"Group\", \"type\": \"nominal\"}, \"column\": {\"field\": \"Performer_Group\", \"header\": {\"labelOrient\": \"bottom\", \"titleOrient\": \"bottom\"}, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Competency_Pillar\", \"type\": \"nominal\"}, {\"field\": \"Performer_Group\", \"type\": \"nominal\"}, {\"field\": \"Mean_Score\", \"format\": \".3f\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"format\": \".2f\"}, \"field\": \"Mean_Score\", \"scale\": {\"domain\": [2.5, 4.0]}, \"title\": \"Mean Competency Score\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Competency_Pillar\", \"sort\": [\"Value Creation for Users\", \"Insight & Decision Sharpness\", \"Curiosity & Experimentation\", \"Social Empathy & Awareness\", \"Growth Drive & Resilience\", \"Quality Delivery Discipline\", \"Forward Thinking & Clarity\", \"Commercial Savvy & Impact\", \"Lead, Inspire & Empower\", \"Synergy & Team Orientation\"], \"title\": \"Competency Pillar\", \"type\": \"nominal\"}}, \"name\": \"view_2\", \"title\": \"Mean Competency Score Comparison\"}], \"params\": [{\"name\": \"param_1\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_1\"]}, {\"name\": \"param_2\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_2\"]}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-374a71f4d4fc99749a3a3c1de3f01cc7\": [{\"Competency_Pillar\": \"Value Creation for Users\", \"Mean_R5\": 3.587949961705387, \"Mean_R12\": 2.9899352267065273, \"Cohens_d\": 0.15966806924355073, \"P_Value\": 2.6675125623522643e-16, \"Significant\": true}, {\"Competency_Pillar\": \"Insight & Decision Sharpness\", \"Mean_R5\": 3.536184210526316, \"Mean_R12\": 2.967588462682129, \"Cohens_d\": 0.15592701719151902, \"P_Value\": 8.710240119673829e-17, \"Significant\": true}, {\"Competency_Pillar\": \"Curiosity & Experimentation\", \"Mean_R5\": 3.4669201520912547, \"Mean_R12\": 2.9517028480382392, \"Cohens_d\": 0.15244581399847562, \"P_Value\": 1.9157862759911007e-21, \"Significant\": true}, {\"Competency_Pillar\": \"Social Empathy & Awareness\", \"Mean_R5\": 3.6720403022670025, \"Mean_R12\": 3.026858892070702, \"Cohens_d\": 0.15017675934271707, \"P_Value\": 4.4915894359424025e-14, \"Significant\": true}, {\"Competency_Pillar\": \"Growth Drive & Resilience\", \"Mean_R5\": 3.4673086670045614, \"Mean_R12\": 2.9625853706819756, \"Cohens_d\": 0.14968427473983742, \"P_Value\": 9.483882260521065e-21, \"Significant\": true}, {\"Competency_Pillar\": \"Quality Delivery Discipline\", \"Mean_R5\": 3.7675716823141334, \"Mean_R12\": 3.0513664472371835, \"Cohens_d\": 0.14087991871367894, \"P_Value\": 7.274717944024791e-12, \"Significant\": true}, {\"Competency_Pillar\": \"Forward Thinking & Clarity\", \"Mean_R5\": 3.59172798782035, \"Mean_R12\": 3.016398330351819, \"Cohens_d\": 0.1358086292656857, \"P_Value\": 4.954213589469318e-13, \"Significant\": true}, {\"Competency_Pillar\": \"Commercial Savvy & Impact\", \"Mean_R5\": 3.4946291560102303, \"Mean_R12\": 3.009371884346959, \"Cohens_d\": 0.12634930769379138, \"P_Value\": 9.638304283930589e-15, \"Significant\": true}, {\"Competency_Pillar\": \"Lead, Inspire & Empower\", \"Mean_R5\": 3.671985815602837, \"Mean_R12\": 3.085066349772232, \"Cohens_d\": 0.1187936667215061, \"P_Value\": 2.980863589785622e-10, \"Significant\": true}, {\"Competency_Pillar\": \"Synergy & Team Orientation\", \"Mean_R5\": 3.612690355329949, \"Mean_R12\": 3.0736863097018645, \"Cohens_d\": 0.1133449334582205, \"P_Value\": 1.1161403423539747e-10, \"Significant\": true}], \"data-f15c316850391f8b09c0452aaf6fbc5f\": [{\"Competency_Pillar\": \"Value Creation for Users\", \"Cohens_d\": 0.15966806924355073, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.587949961705387}, {\"Competency_Pillar\": \"Insight & Decision Sharpness\", \"Cohens_d\": 0.15592701719151902, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.536184210526316}, {\"Competency_Pillar\": \"Curiosity & Experimentation\", \"Cohens_d\": 0.15244581399847562, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.4669201520912547}, {\"Competency_Pillar\": \"Social Empathy & Awareness\", \"Cohens_d\": 0.15017675934271707, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.6720403022670025}, {\"Competency_Pillar\": \"Growth Drive & Resilience\", \"Cohens_d\": 0.14968427473983742, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.4673086670045614}, {\"Competency_Pillar\": \"Quality Delivery Discipline\", \"Cohens_d\": 0.14087991871367894, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.7675716823141334}, {\"Competency_Pillar\": \"Forward Thinking & Clarity\", \"Cohens_d\": 0.1358086292656857, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.59172798782035}, {\"Competency_Pillar\": \"Commercial Savvy & Impact\", \"Cohens_d\": 0.12634930769379138, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.4946291560102303}, {\"Competency_Pillar\": \"Lead, Inspire & Empower\", \"Cohens_d\": 0.1187936667215061, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.671985815602837}, {\"Competency_Pillar\": \"Synergy & Team Orientation\", \"Cohens_d\": 0.1133449334582205, \"Performer_Group\": \"High Performers (R5)\", \"Mean_Score\": 3.612690355329949}, {\"Competency_Pillar\": \"Value Creation for Users\", \"Cohens_d\": 0.15966806924355073, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 2.9899352267065273}, {\"Competency_Pillar\": \"Insight & Decision Sharpness\", \"Cohens_d\": 0.15592701719151902, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 2.967588462682129}, {\"Competency_Pillar\": \"Curiosity & Experimentation\", \"Cohens_d\": 0.15244581399847562, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 2.9517028480382392}, {\"Competency_Pillar\": \"Social Empathy & Awareness\", \"Cohens_d\": 0.15017675934271707, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 3.026858892070702}, {\"Competency_Pillar\": \"Growth Drive & Resilience\", \"Cohens_d\": 0.14968427473983742, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 2.9625853706819756}, {\"Competency_Pillar\": \"Quality Delivery Discipline\", \"Cohens_d\": 0.14087991871367894, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 3.0513664472371835}, {\"Competency_Pillar\": \"Forward Thinking & Clarity\", \"Cohens_d\": 0.1358086292656857, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 3.016398330351819}, {\"Competency_Pillar\": \"Commercial Savvy & Impact\", \"Cohens_d\": 0.12634930769379138, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 3.009371884346959}, {\"Competency_Pillar\": \"Lead, Inspire & Empower\", \"Cohens_d\": 0.1187936667215061, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 3.085066349772232}, {\"Competency_Pillar\": \"Synergy & Team Orientation\", \"Cohens_d\": 0.1133449334582205, \"Performer_Group\": \"Low Performers (R1/R2)\", \"Mean_Score\": 3.0736863097018645}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by Cohen's d descending for visualization consistency\n",
    "df_comp_results = df_comp_results.sort_values(by='Cohens_d', ascending=False)\n",
    "\n",
    "# Get the order of pillars based on Cohen's d for consistent ordering across both charts\n",
    "pillar_order = df_comp_results['Competency_Pillar'].tolist()\n",
    "\n",
    "# --- 2. Visualization 1: Effect Size (Cohen's d) Chart (Primary View) ---\n",
    "\n",
    "chart_cohens_d = alt.Chart(df_comp_results).mark_bar(color='#1f77b4').encode(\n",
    "    # X-axis shows the magnitude of the gap\n",
    "    x=alt.X('Cohens_d', title=\"Performance Gap (Cohen's d)\", scale=alt.Scale(domain=[0.1, 0.17])),\n",
    "    # Y-axis lists the competencies, sorted by the gap size\n",
    "    y=alt.Y('Competency_Pillar', title=\"Competency Pillar\", sort='-x'),\n",
    "    # Tooltip provides detail on hover\n",
    "    tooltip=['Competency_Pillar', alt.Tooltip('Cohens_d', format=\".3f\"), 'P_Value']\n",
    ").properties(\n",
    "    title=\"üèÜ Top 10 Competency Differentiators (Ranked by Effect Size)\"\n",
    ").interactive() # Allows interactive zooming/panning\n",
    "\n",
    "# --- 3. Visualization 2: Mean Score Comparison Chart (Contextual View) ---\n",
    "\n",
    "# Melt the data to compare Mean_R5 and Mean_R12 side-by-side\n",
    "df_means = df_comp_results.melt(\n",
    "    id_vars=['Competency_Pillar', 'Cohens_d'],\n",
    "    value_vars=['Mean_R5', 'Mean_R12'],\n",
    "    var_name='Performer_Group',\n",
    "    value_name='Mean_Score'\n",
    ").replace({'Mean_R5': 'High Performers (R5)', 'Mean_R12': 'Low Performers (R1/R2)'})\n",
    "\n",
    "\n",
    "chart_means = alt.Chart(df_means).mark_bar(opacity=0.8).encode(\n",
    "    # X-axis shows the actual mean score\n",
    "    x=alt.X('Mean_Score', title=\"Mean Competency Score\", axis=alt.Axis(format=\".2f\"), scale=alt.Scale(domain=[2.5, 4.0])),\n",
    "    # Y-axis groups the bars, ordered by the Cohen's d ranking\n",
    "    y=alt.Y('Competency_Pillar', sort=pillar_order, title=\"Competency Pillar\"),\n",
    "    # Color distinguishes the performance groups\n",
    "    color=alt.Color('Performer_Group', title=\"Group\"),\n",
    "    # Columns create separate panels for the two groups\n",
    "    column=alt.Column('Performer_Group', header=alt.Header(titleOrient=\"bottom\", labelOrient=\"bottom\")),\n",
    "    tooltip=['Competency_Pillar', 'Performer_Group', alt.Tooltip('Mean_Score', format=\".3f\")]\n",
    ").properties(\n",
    "    title=\"Mean Competency Score Comparison\"\n",
    ").interactive()\n",
    "\n",
    "# --- 4. Display Charts ---\n",
    "chart_cohens_d | chart_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6cafc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Insights and key finding**\n",
    "\n",
    "> We compared the skill scores of our Highest Performers (Rating 5) against our Lowest Performers (Rating 1 or 2) across ten core competencies.\n",
    "\n",
    "> Finding: The difference between our best and worst employees is not just that the best are slightly better at everything, it‚Äôs that they are significantly better at three specific things that drive business results.\n",
    "\n",
    "> Top 3 Skills to Prioritize for Performance\n",
    "1. Value Creation for Users (The Biggest Gap). Top performers are uniquely skilled at turning effort into visible, tangible impact for clients or internal stakeholders. They don't just complete tasks, they deliver measurable value that matters to the user.\n",
    "    Actionable Insight: If you hire, promote, or coach based on this skill, you will see the fastest results in increasing overall top performance.\n",
    "\n",
    "2. Insight & Decision Sharpness (The Strategic Advantage)\n",
    "    The best employees are significantly better at analyzing complex situations, finding the root cause of a problem, and making correct, fast decisions. They provide clarity where others find confusion.\n",
    "    Actionable Insight: This is the cognitive engine of your top performers. Focus leadership training on strategic thinking, data analysis, and effective risk assessment.\n",
    "\n",
    "3. Curiosity & Experimentation (The Engine of Innovation)\n",
    "    Top performers aren't content with the status quo. They show a stronger drive to learn new things, challenge assumptions, and try new approaches.\n",
    "    Actionable Insight: Foster a culture where testing new ideas (and failing fast) is rewarded. This is the skill that fuels continuous improvement and innovation within the company.\n",
    "\n",
    "> Conclusion: Most employees are already competent at being good team players and delivering basic quality. These skills are the entry ticket to the job, but they will not be the skills that determine who gets the top Rating 5. The key is pushing past the basics into strategic impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405db132",
   "metadata": {},
   "source": [
    "## Psychometric Profiles Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7668220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOP 10 TALENT DIFFERENTIATORS (RATING 5 vs. RATING 1 & 2) ---\n",
      "    Metric  Mean_High  Mean_Low  Cohens_d  P_Value Significant\n",
      "22     gtq     27.954    27.278     0.108    0.027           ‚úÖ\n",
      "11  Papi_O      4.997     4.846     0.059    0.201           ‚ùå\n",
      "15  Papi_T      4.861     5.014    -0.059    0.199           ‚ùå\n",
      "0   Papi_A      4.962     5.090    -0.050    0.269           ‚ùå\n",
      "19  Papi_Z      4.912     5.034    -0.048    0.303           ‚ùå\n",
      "12  Papi_P      5.054     4.944     0.042    0.357           ‚ùå\n",
      "9   Papi_L      5.076     5.173    -0.037    0.419           ‚ùå\n",
      "7   Papi_I      4.953     5.043    -0.035    0.442           ‚ùå\n",
      "14  Papi_S      5.034     4.944     0.034    0.452           ‚ùå\n",
      "20   pauli     60.929    60.287     0.027    0.533           ‚ùå\n"
     ]
    }
   ],
   "source": [
    "# --- Re-define Cohen's d Function ---\n",
    "def calculate_cohens_d(group1, group2):\n",
    "    \"\"\"Calculates Cohen's d for two independent groups.\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    s1, s2 = group1.std(), group2.std()\n",
    "    \n",
    "    # Handle the case where groups are too small or have zero variance\n",
    "    if n1 + n2 - 2 <= 0: return np.nan\n",
    "    \n",
    "    sp = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    if sp == 0: return np.nan\n",
    "        \n",
    "    d = (group1.mean() - group2.mean()) / sp\n",
    "    return d\n",
    "\n",
    "\n",
    "# A. Merge PAPI scores with Rating\n",
    "df_papi_with_rating = df_papi.merge(\n",
    "    df_performance[['employee_id', 'rating']], on='employee_id', how='left'\n",
    ")\n",
    "\n",
    "# B. Merge with Psych scores (creates a temp master table)\n",
    "psych_cols = ['employee_id', 'pauli', 'faxtor', 'iq', 'gtq', 'tiki', 'mbti', 'disc'] \n",
    "df_master_psych_analysis = df_papi_with_rating.merge(\n",
    "    df_psych[psych_cols], on='employee_id', how='left'\n",
    ")\n",
    "\n",
    "# C. Pivot PAPI scores (from long to wide format)\n",
    "df_papi_pivoted = df_master_psych_analysis.groupby(['employee_id', 'scale_code'])['score'].mean().unstack()\n",
    "\n",
    "# D. Prepare psych/rating data\n",
    "df_psych_with_rating = df_master_psych_analysis[\n",
    "    ['employee_id', 'rating', 'pauli', 'faxtor', 'iq', 'gtq', 'tiki']\n",
    "].drop_duplicates(subset=['employee_id', 'rating'])\n",
    "\n",
    "# E. Final Merge to create df_analysis\n",
    "df_analysis = df_papi_pivoted.merge(\n",
    "    df_psych_with_rating,\n",
    "    on='employee_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "PAPI_SCALES = [col for col in df_analysis.columns if col.startswith('Papi_')]\n",
    "PSYCH_METRICS = ['pauli', 'iq', 'gtq', 'tiki'] # Numeric psych metrics\n",
    "ALL_METRICS_TO_TEST = PAPI_SCALES + PSYCH_METRICS\n",
    "\n",
    "test_results_r5_r12 = []\n",
    "HIGH_PERFORMER_RATING = 5.0\n",
    "LOW_PERFORMER_RATINGS = [1.0, 2.0]  # Expanded to include ratings 1, 2, 3, and 4\n",
    "MIN_SAMPLE_SIZE = 5\n",
    "\n",
    "df_low_performers = df_analysis[df_analysis['rating'].isin(LOW_PERFORMER_RATINGS)]\n",
    "\n",
    "for metric in ALL_METRICS_TO_TEST:\n",
    "    if metric not in df_analysis.columns:\n",
    "        continue\n",
    "        \n",
    "    high = df_analysis[df_analysis['rating'] == HIGH_PERFORMER_RATING][metric].dropna()\n",
    "    low = df_low_performers[metric].dropna() \n",
    "    \n",
    "    if len(high) >= MIN_SAMPLE_SIZE and len(low) >= MIN_SAMPLE_SIZE:\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_ind(high, low, equal_var=False) \n",
    "        cohens_d = calculate_cohens_d(high, low)\n",
    "        \n",
    "        test_results_r5_r12.append({\n",
    "            'Metric': metric,\n",
    "            'Mean_High': high.mean(),\n",
    "            'Mean_Low': low.mean(),\n",
    "            'Difference': high.mean() - low.mean(),\n",
    "            'P_Value': p_value,\n",
    "            'Cohens_d': cohens_d,\n",
    "            'Significant': p_value < 0.05\n",
    "        })\n",
    "\n",
    "df_r5_r12_results = pd.DataFrame(test_results_r5_r12)\n",
    "\n",
    "# Sort and display the results\n",
    "df_r5_r12_top_effects = (\n",
    "    df_r5_r12_results\n",
    "    .iloc[df_r5_r12_results['Cohens_d'].abs().argsort()[::-1]]\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\n--- TOP 10 TALENT DIFFERENTIATORS (RATING 5 vs. RATING 1 & 2) ---\")\n",
    "df_r5_r12_top_effects['Significant'] = df_r5_r12_top_effects['Significant'].apply(lambda x: '‚úÖ' if x else '‚ùå')\n",
    "print(df_r5_r12_top_effects[['Metric', 'Mean_High', 'Mean_Low', 'Cohens_d', 'P_Value', 'Significant']].round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CATEGORICAL PSYCHOMETRICS (R5 vs R1/R2) Chi-Squared Test ---\n",
      "  Metric      Chi2  P_Value Dominant_Category_in_R5  R5_Category_Pct Significant\n",
      "0   disc  556.3072      0.0                      CI          10.6098           ‚úÖ\n",
      "1   mbti  386.8177      0.0                    ENFP           8.5080           ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "CATEGORICAL_PSYCH_METRICS = ['disc', 'mbti']\n",
    "HIGH_PERFORMER_RATING = 5.0\n",
    "\n",
    "# CRITICAL CORRECTION: Focus on the true \"Low\" performers to maximize contrast.\n",
    "LOW_PERFORMER_RATINGS = [1.0, 2.0] \n",
    "\n",
    "categorical_test_results = []\n",
    "\n",
    "# Create the two comparative groups\n",
    "df_high = df_master_psych_analysis[df_master_psych_analysis['rating'] == HIGH_PERFORMER_RATING].copy()\n",
    "df_low = df_master_psych_analysis[df_master_psych_analysis['rating'].isin(LOW_PERFORMER_RATINGS)].copy()\n",
    "\n",
    "# Combine only the two contrasting groups\n",
    "df_combined = pd.concat([df_high, df_low])\n",
    "\n",
    "# Loop through each categorical metric\n",
    "for metric in CATEGORICAL_PSYCH_METRICS:\n",
    "    if metric not in df_combined.columns:\n",
    "        continue\n",
    "        \n",
    "    # Drop NaNs and ensure we have enough data\n",
    "    df_temp = df_combined[[metric, 'rating']].dropna()\n",
    "    \n",
    "    # Create the contingency table\n",
    "    contingency_table = pd.crosstab(df_temp[metric], df_temp['rating'])\n",
    "    \n",
    "    # Ensure there are at least two rows and two columns to run the test\n",
    "    if contingency_table.shape[0] > 1 and contingency_table.shape[1] > 1:\n",
    "        \n",
    "        # --- Chi-Squared Test ---\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "        \n",
    "        # Calculate the distribution of categories within the High Performer group (Rating 5.0)\n",
    "        high_dist = contingency_table[HIGH_PERFORMER_RATING].div(\n",
    "            contingency_table[HIGH_PERFORMER_RATING].sum()\n",
    "        ) * 100\n",
    "        \n",
    "        # Find the category most prevalent among High Performers\n",
    "        top_category = high_dist.idxmax()\n",
    "        top_percentage = high_dist.max()\n",
    "        \n",
    "        categorical_test_results.append({\n",
    "            'Metric': metric,\n",
    "            'Chi2': chi2,\n",
    "            'P_Value': p_value,\n",
    "            'Dominant_Category_in_R5': top_category,\n",
    "            'R5_Category_Pct': top_percentage,\n",
    "            'Significant': p_value < 0.05\n",
    "        })\n",
    "\n",
    "df_categorical_results = pd.DataFrame(categorical_test_results)\n",
    "\n",
    "print(\"\\n--- CATEGORICAL PSYCHOMETRICS (R5 vs R1/R2) Chi-Squared Test ---\")\n",
    "df_categorical_results['Significant'] = df_categorical_results['Significant'].apply(lambda x: '‚úÖ' if x else '‚ùå')\n",
    "\n",
    "# Sort by P_Value to show the most significant results first\n",
    "print(df_categorical_results.sort_values('P_Value').round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061cf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOP 15 PREDICTIVE DRIVERS OF RATING 5 (Logistic Regression) ---\n",
      "Interpretation: Positive Coeff = Trait increases the ODDS of achieving Rating 5.\n",
      "Interpretation: Negative Coeff = Trait decreases the ODDS of achieving Rating 5.\n",
      "       Metric  Coefficient (Scaled)\n",
      "36  mbti_ENTJ               -0.9736\n",
      "49  mbti_ISTP                0.7301\n",
      "40  mbti_ESTJ               -0.6278\n",
      "24    disc_CI                0.5343\n",
      "38  mbti_ESFJ                0.5168\n",
      "25    disc_CS               -0.4986\n",
      "28    disc_DS                0.4528\n",
      "31    disc_IS               -0.4114\n",
      "46  mbti_ISFJ                0.3236\n",
      "47  mbti_ISFP                0.2666\n",
      "35  mbti_ENFP                0.2622\n",
      "44  mbti_INTJ                0.2102\n",
      "26    disc_DC                0.2044\n",
      "32    disc_SC               -0.1968\n",
      "17     Papi_W               -0.1860\n"
     ]
    }
   ],
   "source": [
    "# Recreate the df_analysis structure, including all categorical metrics for encoding\n",
    "df_master = df_master_psych_analysis.copy()\n",
    "\n",
    "# A. Create the PAPI pivoted structure\n",
    "df_papi_pivoted = df_master.groupby(['employee_id', 'scale_code'])['score'].mean().unstack()\n",
    "\n",
    "# B. Prepare Psych and Rating data (including categoricals for encoding)\n",
    "psych_cols_for_merge = ['employee_id', 'rating', 'pauli', 'iq', 'gtq', 'tiki', 'disc', 'mbti']\n",
    "df_psych_with_rating = df_master[psych_cols_for_merge].drop_duplicates(subset=['employee_id', 'rating'])\n",
    "\n",
    "# C. Final Merge to create the base analysis table\n",
    "df_analysis_base = df_papi_pivoted.merge(\n",
    "    df_psych_with_rating,\n",
    "    on='employee_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# --- MODEL SETUP ---\n",
    "\n",
    "# 1. Define Target Variable (1 = High Performer, 0 = Non-High Performer)\n",
    "df_analysis_base['is_high_performer'] = np.where(df_analysis_base['rating'] == 5.0, 1, 0)\n",
    "TARGET = 'is_high_performer'\n",
    "\n",
    "# 2. Define Features\n",
    "NUMERIC_FEATURES = [col for col in df_papi_pivoted.columns.tolist() if col in df_analysis_base.columns] + ['pauli', 'iq', 'gtq', 'tiki']\n",
    "CATEGORICAL_FEATURES = ['disc', 'mbti'] # Include the significant categorical features\n",
    "\n",
    "# 3. Handle Missing Values\n",
    "# Drop rows with any missing values in the features used for the model\n",
    "FEATURES_TO_CHECK = NUMERIC_FEATURES + CATEGORICAL_FEATURES + [TARGET]\n",
    "df_model_ready = df_analysis_base[FEATURES_TO_CHECK].dropna()\n",
    "\n",
    "\n",
    "# 4. One-Hot Encode Categorical Features (CRUCIAL STEP)\n",
    "# Convert DISC and MBTI into binary (0 or 1) columns for the model\n",
    "df_encoded = pd.get_dummies(df_model_ready, columns=CATEGORICAL_FEATURES, drop_first=True)\n",
    "\n",
    "\n",
    "# 5. Define X and Y for the model\n",
    "# X = All encoded numeric and categorical features\n",
    "X = df_encoded.drop(columns=[TARGET])\n",
    "# Y = The binary target\n",
    "y = df_encoded[TARGET]\n",
    "\n",
    "\n",
    "# 6. Standardize Numeric Features (CRUCIAL for comparing coefficients)\n",
    "# Standardize only the numeric columns to prevent features with larger scales (like IQ) from dominating\n",
    "scaler = StandardScaler()\n",
    "X[NUMERIC_FEATURES] = scaler.fit_transform(X[NUMERIC_FEATURES])\n",
    "\n",
    "\n",
    "# 7. Run Logistic Regression\n",
    "# Use L2 penalty (Ridge) for robust coefficient estimation\n",
    "model = LogisticRegression(penalty='l2', solver='liblinear', random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 8. Extract and Sort Coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Metric': X.columns,\n",
    "    'Coefficient (Scaled)': model.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort by absolute magnitude of the coefficient to find the strongest drivers\n",
    "coefficients['Abs_Coeff'] = coefficients['Coefficient (Scaled)'].abs()\n",
    "df_model_drivers = coefficients.sort_values('Abs_Coeff', ascending=False).drop(columns=['Abs_Coeff']).head(15)\n",
    "\n",
    "\n",
    "print(\"\\n--- TOP 15 PREDICTIVE DRIVERS OF RATING 5 (Logistic Regression) ---\")\n",
    "print(\"Interpretation: Positive Coeff = Trait increases the ODDS of achieving Rating 5.\")\n",
    "print(\"Interpretation: Negative Coeff = Trait decreases the ODDS of achieving Rating 5.\")\n",
    "print(df_model_drivers.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f493bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-6b162ac614d14f42a5c6377363ef8131.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-6b162ac614d14f42a5c6377363ef8131.vega-embed details,\n",
       "  #altair-viz-6b162ac614d14f42a5c6377363ef8131.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-6b162ac614d14f42a5c6377363ef8131\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6b162ac614d14f42a5c6377363ef8131\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6b162ac614d14f42a5c6377363ef8131\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-42990c68f268a11fa16682fcd3437a6f\"}, \"mark\": {\"type\": \"rule\", \"color\": \"black\", \"strokeDash\": [3, 3]}, \"encoding\": {\"x\": {\"field\": \"zero\", \"type\": \"quantitative\"}}, \"name\": \"view_3\"}, {\"data\": {\"name\": \"data-e256c0c885098a9f66e5c8b0a27e0272\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Direction\", \"scale\": {\"domain\": [\"Positive Driver (R5 Increase)\", \"Negative Suppressor (R5 Decrease)\"], \"range\": [\"#1f77b4\", \"#d62728\"]}, \"title\": \"Impact on Odds of Rating 5\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Metric\", \"type\": \"nominal\"}, {\"field\": \"Coefficient (Scaled)\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"Direction\", \"type\": \"nominal\"}], \"x\": {\"field\": \"Coefficient (Scaled)\", \"title\": \"Standardized Predictive Strength (Coefficient)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Metric\", \"sort\": {\"field\": \"Coefficient (Scaled)\", \"op\": \"mean\", \"order\": \"ascending\"}, \"title\": \"PAPI/Psych Metric\", \"type\": \"nominal\"}}}], \"params\": [{\"name\": \"param_3\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_3\"]}], \"title\": \"Top Drivers and Suppressors of High Performance (Rating 5)\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-42990c68f268a11fa16682fcd3437a6f\": [{\"zero\": 0}], \"data-e256c0c885098a9f66e5c8b0a27e0272\": [{\"Metric\": \"mbti_ENTJ\", \"Coefficient (Scaled)\": -0.9736381800830258, \"Direction\": \"Negative Suppressor (R5 Decrease)\"}, {\"Metric\": \"mbti_ESTJ\", \"Coefficient (Scaled)\": -0.6277820621534502, \"Direction\": \"Negative Suppressor (R5 Decrease)\"}, {\"Metric\": \"disc_CS\", \"Coefficient (Scaled)\": -0.49859044619235565, \"Direction\": \"Negative Suppressor (R5 Decrease)\"}, {\"Metric\": \"disc_IS\", \"Coefficient (Scaled)\": -0.4113718308061414, \"Direction\": \"Negative Suppressor (R5 Decrease)\"}, {\"Metric\": \"disc_SC\", \"Coefficient (Scaled)\": -0.19683337504441176, \"Direction\": \"Negative Suppressor (R5 Decrease)\"}, {\"Metric\": \"Papi_W\", \"Coefficient (Scaled)\": -0.18604808203674295, \"Direction\": \"Negative Suppressor (R5 Decrease)\"}, {\"Metric\": \"disc_DC\", \"Coefficient (Scaled)\": 0.20438578420719655, \"Direction\": \"Positive Driver (R5 Increase)\"}, {\"Metric\": \"mbti_INTJ\", \"Coefficient (Scaled)\": 0.2101941549745634, \"Direction\": \"Positive Driver (R5 Increase)\"}, {\"Metric\": \"mbti_ENFP\", \"Coefficient (Scaled)\": 0.2621708105463439, \"Direction\": \"Positive Driver (R5 Increase)\"}, {\"Metric\": \"mbti_ISFP\", \"Coefficient (Scaled)\": 0.2666264580918205, \"Direction\": \"Positive Driver (R5 Increase)\"}, {\"Metric\": \"mbti_ISFJ\", \"Coefficient (Scaled)\": 0.32360272570428694, \"Direction\": \"Positive Driver (R5 Increase)\"}, {\"Metric\": \"disc_DS\", \"Coefficient (Scaled)\": 0.4527760483827885, \"Direction\": \"Positive Driver (R5 Increase)\"}, {\"Metric\": \"mbti_ESFJ\", \"Coefficient (Scaled)\": 0.5167610608415792, \"Direction\": \"Positive Driver (R5 Increase)\"}, {\"Metric\": \"disc_CI\", \"Coefficient (Scaled)\": 0.5342514670230368, \"Direction\": \"Positive Driver (R5 Increase)\"}, {\"Metric\": \"mbti_ISTP\", \"Coefficient (Scaled)\": 0.7300574544244085, \"Direction\": \"Positive Driver (R5 Increase)\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drivers = df_model_drivers.copy()\n",
    "\n",
    "# Create a color category based on the sign of the coefficient\n",
    "df_drivers['Direction'] = df_drivers['Coefficient (Scaled)'].apply(lambda x: 'Positive Driver (R5 Increase)' if x > 0 else 'Negative Suppressor (R5 Decrease)')\n",
    "\n",
    "# Sort by coefficient magnitude for the chart presentation\n",
    "df_drivers = df_drivers.sort_values(by='Coefficient (Scaled)', ascending=True)\n",
    "\n",
    "# Using the prepared df_drivers DataFrame\n",
    "base = alt.Chart(df_drivers).encode(\n",
    "    # Y-axis lists the metrics, sorted by the coefficient value\n",
    "    y=alt.Y('Metric', sort=alt.EncodingSortField(field='Coefficient (Scaled)', op='mean', order='ascending'), title=\"PAPI/Psych Metric\"),\n",
    "    # Tooltip provides detail on hover\n",
    "    tooltip=['Metric', alt.Tooltip('Coefficient (Scaled)', format=\".4f\"), 'Direction']\n",
    ")\n",
    "\n",
    "# 1. Bar Marks (The Coefficients)\n",
    "bars = base.mark_bar().encode(\n",
    "    x=alt.X('Coefficient (Scaled)', title=\"Standardized Predictive Strength (Coefficient)\"),\n",
    "    # Color based on the direction (Positive/Negative)\n",
    "    color=alt.Color('Direction', \n",
    "                    scale=alt.Scale(domain=['Positive Driver (R5 Increase)', 'Negative Suppressor (R5 Decrease)'], \n",
    "                                    range=['#1f77b4', '#d62728']), \n",
    "                    title=\"Impact on Odds of Rating 5\")\n",
    ")\n",
    "\n",
    "# 2. Zero Line (Reference)\n",
    "zero_line = alt.Chart(pd.DataFrame({'zero': [0]})).mark_rule(color='black', strokeDash=[3, 3]).encode(\n",
    "    x='zero:Q'\n",
    ")\n",
    "\n",
    "# Combine and finalize the chart\n",
    "chart_altair = (zero_line + bars).properties(\n",
    "    title=\"Top Drivers and Suppressors of High Performance (Rating 5)\"\n",
    ").interactive() # Allows interactive zooming/panning\n",
    "\n",
    "chart_altair # Display the chart in the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf52e5d",
   "metadata": {},
   "source": [
    "**Insights and Key Findings**\n",
    "\n",
    "> We compared the cognitive ability and personality profiles of our Highest Performers (Rating 5) against our Lowest Performers (Rating 1 or 2).\n",
    "\n",
    "> Finding: The difference between our best and worst employees is not based on general friendliness or basic work ethic; it‚Äôs that the top performers are significantly better endowed with raw intellectual capacity and possess specific analytical personality traits.\n",
    "\n",
    "> Top 3 Traits to Prioritize for Performance\n",
    "\n",
    "1. General Talent Quotient (GTQ) / Cognitive Power (The Gatekeeper). Top performers possess significantly higher GTQ scores, meaning they have superior analytical and complex problem-solving abilities. Actionable Insight: Cognitive ability is the essential entry ticket. When hiring or promoting for critical roles, GTQ should be the most important, non-negotiable metric. No amount of personality fit can fully compensate for a lack of intellectual horsepower.\n",
    "\n",
    "2. MBTI: ISTP (The Analytical Executioner).This reserved, practical, and analytical profile is the single strongest positive personality driver of R5 success. Top performers are often \"mechanics\", focused on mastery, troubleshooting, and executing technical solutions with precision. Actionable Insight: When building high-stakes project teams, prioritize individuals who can demonstrate deep technical mastery and a calm, analytical approach to problem-solving. Success is driven by reliable specialists, not just general managers.\n",
    "\n",
    "3. DISC: CI (Conscientiousness/Influence) (The Methodical Achiever).This profile combines a powerful focus on accuracy and high standards with an ability to communicate effectively. They ensure rigorous quality while still engaging stakeholders. Actionable Insight: This trait indicates that high performance demands a combination of methodical rigor and diplomatic engagement. Coach employees to uphold strict quality standards while effectively influencing peers and clients.\n",
    "\n",
    "> Conclusion: The recipe for a Rating 5 is clear: You need High Cognitive Power GTQ paired with the Personality of a Master Executor ISTP or CI. Many common personality traits (like agreeableness or extraversion) do not independently influence top performance. Furthermore, traditionally dominant profiles MBTI: ENTJ and ESTJ are surprisingly less likely to achieve R5, suggesting the key is disciplined execution, not aggressive management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4bffa8",
   "metadata": {},
   "source": [
    "## Behavioral Strengths Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "906cf6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dynamically selecting top N strengths (based on OR)...\n",
      "   Selected TOP_STRENGTHS (5): ['Futuristic', 'Learner', 'Relator', 'Intellection', 'Activator']\n",
      "2. Dynamically selecting top N competencies (based on Cohen's d)...\n",
      "   Selected TOP_COMPETENCIES (2): ['Papi_P', 'Papi_S']\n",
      "\n",
      "4. Fitting Final Integrated Logistic Regression...\n",
      "Fitting Complete.\n",
      "\n",
      "--- üèÜ Final Drivers of Rating 5 (Integrated Model) ---\n",
      "These are the unique, independent predictors of R5 performance.\n",
      "      Metric  Odds_Ratio  P_Value Significant         Significance_Color\n",
      "  Futuristic      2.2358   0.0050           ‚úÖ     Significant (P < 0.05)\n",
      "     Learner      1.8201   0.0405           ‚úÖ     Significant (P < 0.05)\n",
      "   Activator      1.6444   0.1226           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "     Relator      1.6335   0.1139           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "   mbti_ISTP      1.4548   0.2250           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "Intellection      1.3348   0.3640           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "      Papi_P      1.0375   0.2023           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "         gtq      1.0048   0.5029           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "     disc_CI      0.9709   0.9197           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "      Papi_S      0.9656   0.2183           ‚ùå Not Significant (P ‚â• 0.05)\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "N_STRENGTHS = 5    # Number of top strengths to include\n",
    "N_COMPETENCIES = 2 # Number of top competencies to include\n",
    "TOP_MBTI = ['ISTP']  # Manually selected strong predictors\n",
    "TOP_DISC = ['CI']    # Manually selected strong predictors\n",
    "NUMERIC_PREDICTORS = ['gtq']\n",
    "\n",
    "# Helper function to clean column names\n",
    "def sanitize_column_name(name):\n",
    "    return name.replace(' ', '_').replace('&', 'and')\n",
    "\n",
    "# --- 1. Master Data Integration and Target Creation ---\n",
    "\n",
    "# Combine Performance and PAPI/Competency Data to get base ratings and scores\n",
    "df_master_comp = df_papi.merge(\n",
    "    df_performance[['employee_id', 'rating', 'year']],\n",
    "    on='employee_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Use the latest rating for performance status\n",
    "df_master_rating = df_master_comp.sort_values(['employee_id','year'], ascending=[True,False]).drop_duplicates('employee_id')[['employee_id', 'rating']]\n",
    "\n",
    "# Merge with Psychometric Data (IQ, MBTI, DISC, GTQ)\n",
    "df_master = df_master_rating.merge(\n",
    "    df_psych[['employee_id', 'gtq', 'mbti', 'disc']].drop_duplicates(),\n",
    "    on='employee_id',\n",
    "    how='inner' # Only include employees with both rating and psych data\n",
    ")\n",
    "\n",
    "# Create the Binary Target Variable\n",
    "TARGET_VARIABLE = 'is_R5'\n",
    "df_master[TARGET_VARIABLE] = np.where(df_master['rating'] == 5.0, 1, 0)\n",
    "df_master = df_master.drop_duplicates(subset=['employee_id']).reset_index(drop=True)\n",
    "\n",
    "# --- 2. DYNAMIC PREDICTOR SELECTION ---\n",
    "\n",
    "# 2A. Dynamic Selection of TOP_STRENGTHS (Based on Odds Ratio)\n",
    "print(\"1. Dynamically selecting top N strengths (based on OR)...\")\n",
    "df_perf_latest = df_performance.sort_values(['employee_id','year'], ascending=[True,False]).drop_duplicates('employee_id')[['employee_id', 'rating']]\n",
    "df_strengths_with_rating = df_strengths[df_strengths['rank'] < 3].merge(\n",
    "    df_perf_latest, on='employee_id', how='left'\n",
    ")\n",
    "df_strengths_with_rating['is_R5'] = np.where(df_strengths_with_rating['rating'] == 5.0, 1, 0)\n",
    "df_strengths_with_rating['is_low_performer'] = np.where(df_strengths_with_rating['rating'].isin([1.0, 2.0]), 1, 0)\n",
    "\n",
    "df_analysis = df_strengths_with_rating[\n",
    "    (df_strengths_with_rating['is_R5'] == 1) | (df_strengths_with_rating['is_low_performer'] == 1)\n",
    "].copy()\n",
    "\n",
    "employee_master_strengths = df_analysis[['employee_id', 'is_R5']].drop_duplicates()\n",
    "TOTAL_R5 = employee_master_strengths['is_R5'].sum()\n",
    "TOTAL_NONR5 = len(employee_master_strengths) - TOTAL_R5\n",
    "\n",
    "df_theme_presence = df_analysis.groupby(['employee_id', 'theme']).size().unstack(fill_value=0)\n",
    "df_theme_presence = (df_theme_presence > 0).astype(int) \n",
    "df_theme_presence = employee_master_strengths.merge(\n",
    "    df_theme_presence, on='employee_id', how='inner'\n",
    ").drop(columns=['employee_id'])\n",
    "\n",
    "odds_ratio_results = []\n",
    "theme_list = [col for col in df_theme_presence.columns if col != 'is_R5']\n",
    "for theme in theme_list:\n",
    "    a = df_theme_presence[(df_theme_presence['is_R5'] == 1) & (df_theme_presence[theme] == 1)].shape[0]\n",
    "    b = df_theme_presence[(df_theme_presence['is_R5'] == 0) & (df_theme_presence[theme] == 1)].shape[0]\n",
    "    c = TOTAL_R5 - a\n",
    "    d = TOTAL_NONR5 - b\n",
    "    if a == 0 or b == 0 or c == 0 or d == 0:\n",
    "        a += 0.5; b += 0.5; c += 0.5; d += 0.5\n",
    "    odds_ratio = (a * d) / (b * c)\n",
    "    odds_ratio_results.append({'Theme': theme, 'Odds_Ratio': odds_ratio})\n",
    "\n",
    "df_odds_ratio = pd.DataFrame(odds_ratio_results)\n",
    "TOP_STRENGTHS = df_odds_ratio.sort_values(by='Odds_Ratio', ascending=False).head(N_STRENGTHS)['Theme'].tolist()\n",
    "print(f\"   Selected TOP_STRENGTHS ({N_STRENGTHS}): {TOP_STRENGTHS}\")\n",
    "\n",
    "\n",
    "# 2B. Dynamic Selection of TOP_COMPETENCIES (Based on Cohen's d)\n",
    "print(\"2. Dynamically selecting top N competencies (based on Cohen's d)...\")\n",
    "df_all_scores = df_master_comp[['employee_id', 'scale_code', 'score']].merge(\n",
    "    df_master[['employee_id', 'is_R5']].drop_duplicates(),\n",
    "    on='employee_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "competency_d_results = []\n",
    "all_competencies = df_all_scores['scale_code'].unique()\n",
    "\n",
    "for comp in all_competencies:\n",
    "    scores_r5 = df_all_scores[(df_all_scores['scale_code'] == comp) & (df_all_scores['is_R5'] == 1)]['score'].dropna()\n",
    "    scores_not_r5 = df_all_scores[(df_all_scores['scale_code'] == comp) & (df_all_scores['is_R5'] == 0)]['score'].dropna()\n",
    "    \n",
    "    if len(scores_r5) < 2 or len(scores_not_r5) < 2:\n",
    "        continue\n",
    "\n",
    "    mean_diff = scores_r5.mean() - scores_not_r5.mean()\n",
    "    n1, n2 = len(scores_r5), len(scores_not_r5)\n",
    "    std1, std2 = scores_r5.std(), scores_not_r5.std()\n",
    "    \n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    competency_d_results.append({'Competency': comp, 'Cohens_D': cohens_d})\n",
    "\n",
    "df_competency_ranking = pd.DataFrame(competency_d_results)\n",
    "TOP_COMPETENCIES = df_competency_ranking.iloc[\n",
    "    df_competency_ranking['Cohens_D'].abs().argsort()[::-1]\n",
    "].head(N_COMPETENCIES)['Competency'].tolist()\n",
    "print(f\"   Selected TOP_COMPETENCIES ({N_COMPETENCIES}): {TOP_COMPETENCIES}\")\n",
    "\n",
    "\n",
    "# --- 3. FINAL FEATURE PREPARATION (Pivoting and Merging) ---\n",
    "\n",
    "# 3A. Competency Score Preparation (Pivoting)\n",
    "df_competency_pivot = df_master_comp.pivot_table(\n",
    "    index='employee_id',\n",
    "    columns='scale_code',\n",
    "    values='score',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "comp_name_map = {\n",
    "    col_orig: sanitize_column_name(col_orig) \n",
    "    for col_orig in df_competency_pivot.columns \n",
    "    if col_orig in TOP_COMPETENCIES\n",
    "}\n",
    "\n",
    "df_competency_scores = df_competency_pivot[['employee_id'] + list(comp_name_map.keys())].rename(columns=comp_name_map)\n",
    "df_master = df_master.merge(df_competency_scores, on='employee_id', how='left')\n",
    "\n",
    "# 3B. Strengths Dummy Preparation\n",
    "df_strength_pivot = df_strengths[df_strengths['rank'] < 3].assign(value=1).pivot_table(\n",
    "    index='employee_id', \n",
    "    columns='theme', \n",
    "    values='value', \n",
    "    aggfunc='max'\n",
    ").fillna(0).reset_index()\n",
    "\n",
    "df_strength_dummies = df_strength_pivot[['employee_id'] + TOP_STRENGTHS]\n",
    "df_master = df_master.merge(df_strength_dummies, on='employee_id', how='left').fillna(0) \n",
    "\n",
    "# 3C. Categorical Dummies\n",
    "for mbti_type in TOP_MBTI:\n",
    "    df_master[f'mbti_{mbti_type}'] = np.where(df_master['mbti'] == mbti_type, 1, 0)\n",
    "\n",
    "for disc_type in TOP_DISC:\n",
    "    df_master[f'disc_{disc_type}'] = np.where(df_master['disc'] == disc_type, 1, 0)\n",
    "\n",
    "\n",
    "# --- 4. INTEGRATED LOGISTIC REGRESSION ---\n",
    "\n",
    "# Compile Final Predictor List\n",
    "COMPETENCY_PREDICTORS = [sanitize_column_name(c) for c in TOP_COMPETENCIES]\n",
    "STRENGTH_PREDICTORS = TOP_STRENGTHS\n",
    "MBTI_DISC_PREDICTORS = [f'mbti_{mbti}' for mbti in TOP_MBTI] + [f'disc_{disc}' for disc in TOP_DISC]\n",
    "\n",
    "ALL_PREDICTORS = NUMERIC_PREDICTORS + COMPETENCY_PREDICTORS + STRENGTH_PREDICTORS + MBTI_DISC_PREDICTORS\n",
    "\n",
    "# Prepare Data (Dropping NaNs is critical for regression)\n",
    "df_final_model = df_master[[TARGET_VARIABLE] + ALL_PREDICTORS].dropna()\n",
    "\n",
    "Y = df_final_model[TARGET_VARIABLE]\n",
    "X = df_final_model[ALL_PREDICTORS]\n",
    "X = sm.add_constant(X, prepend=False)\n",
    "\n",
    "# Run the Model\n",
    "print(\"\\n4. Fitting Final Integrated Logistic Regression...\")\n",
    "model = sm.Logit(Y, X)\n",
    "result = model.fit(disp=False)\n",
    "print(\"Fitting Complete.\")\n",
    "\n",
    "# --- 5. EXTRACT AND VISUALIZE RESULTS ---\n",
    "\n",
    "# Calculate Odds Ratios and P-Values\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': result.params.index,\n",
    "    'Odds_Ratio': np.exp(result.params.values),\n",
    "    'P_Value': result.pvalues.values\n",
    "})\n",
    "\n",
    "results_df = results_df[results_df['Metric'] != 'const']\n",
    "\n",
    "# Determine Significance and Color for Visualization\n",
    "results_df['Significant'] = np.where(results_df['P_Value'] < 0.05, '‚úÖ', '‚ùå')\n",
    "results_df['Significance_Color'] = np.where(results_df['P_Value'] < 0.05, 'Significant (P < 0.05)', 'Not Significant (P ‚â• 0.05)')\n",
    "\n",
    "df_final_ranking = results_df.sort_values(by='Odds_Ratio', ascending=False).round(4)\n",
    "\n",
    "print(\"\\n--- üèÜ Final Drivers of Rating 5 (Integrated Model) ---\")\n",
    "print(\"These are the unique, independent predictors of R5 performance.\")\n",
    "print(df_final_ranking.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb76949e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2957d9acef1f48d1a0d8c0cc710324c4.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2957d9acef1f48d1a0d8c0cc710324c4.vega-embed details,\n",
       "  #altair-viz-2957d9acef1f48d1a0d8c0cc710324c4.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2957d9acef1f48d1a0d8c0cc710324c4\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2957d9acef1f48d1a0d8c0cc710324c4\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2957d9acef1f48d1a0d8c0cc710324c4\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-361ea6f3ceae4bc75d7d8c51b7055154\"}, \"mark\": {\"type\": \"rule\", \"color\": \"black\", \"strokeDash\": [3, 3]}, \"encoding\": {\"x\": {\"field\": \"OR_1\", \"type\": \"quantitative\"}}}, {\"data\": {\"name\": \"data-ea2f20254ccdb320a604aa3a3144fb8c\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Significance_Color\", \"scale\": {\"domain\": [\"Significant (P < 0.05)\", \"Not Significant (P \\u2265 0.05)\"], \"range\": [\"#2ca02c\", \"#ff7f0e\"]}, \"title\": \"Statistical Significance\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Metric\", \"type\": \"nominal\"}, {\"field\": \"Odds_Ratio\", \"format\": \".3f\", \"type\": \"quantitative\"}, {\"field\": \"P_Value\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"Significant\", \"type\": \"nominal\"}], \"x\": {\"field\": \"Odds_Ratio\", \"title\": \"Odds Ratio (e^Coefficient)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Metric\", \"sort\": {\"field\": \"Odds_Ratio\", \"op\": \"mean\", \"order\": \"descending\"}, \"title\": \"Predictor Metric\", \"type\": \"nominal\"}}, \"name\": \"view_4\", \"title\": \"Integrated Model: Predictive Odds Ratio for Rating 5\"}], \"params\": [{\"name\": \"param_4\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_4\"]}], \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-361ea6f3ceae4bc75d7d8c51b7055154\": [{\"OR_1\": 1}], \"data-ea2f20254ccdb320a604aa3a3144fb8c\": [{\"Metric\": \"Futuristic\", \"Odds_Ratio\": 2.2358, \"P_Value\": 0.005, \"Significant\": \"\\u2705\", \"Significance_Color\": \"Significant (P < 0.05)\"}, {\"Metric\": \"Learner\", \"Odds_Ratio\": 1.8201, \"P_Value\": 0.0405, \"Significant\": \"\\u2705\", \"Significance_Color\": \"Significant (P < 0.05)\"}, {\"Metric\": \"Activator\", \"Odds_Ratio\": 1.6444, \"P_Value\": 0.1226, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"Relator\", \"Odds_Ratio\": 1.6335, \"P_Value\": 0.1139, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"mbti_ISTP\", \"Odds_Ratio\": 1.4548, \"P_Value\": 0.225, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"Intellection\", \"Odds_Ratio\": 1.3348, \"P_Value\": 0.364, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"Papi_P\", \"Odds_Ratio\": 1.0375, \"P_Value\": 0.2023, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"gtq\", \"Odds_Ratio\": 1.0048, \"P_Value\": 0.5029, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"disc_CI\", \"Odds_Ratio\": 0.9709, \"P_Value\": 0.9197, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"Papi_S\", \"Odds_Ratio\": 0.9656, \"P_Value\": 0.2183, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Visualization (Odds Ratio Ranking) ---\n",
    "\n",
    "# Set a baseline for the Y-axis (OR = 1.0)\n",
    "base = alt.Chart(df_final_ranking).encode(\n",
    "    y=alt.Y('Metric', sort=alt.EncodingSortField(field='Odds_Ratio', op='mean', order='descending'), title='Predictor Metric'),\n",
    "    tooltip=['Metric', alt.Tooltip('Odds_Ratio', format='.3f'), alt.Tooltip('P_Value', format='.4f'), 'Significant']\n",
    ").properties(\n",
    "    title='Integrated Model: Predictive Odds Ratio for Rating 5'\n",
    ").interactive()\n",
    "\n",
    "# Bar Chart for Magnitude (Odds Ratio)\n",
    "bars = base.mark_bar().encode(\n",
    "    x=alt.X('Odds_Ratio', title='Odds Ratio (e^Coefficient)'),\n",
    "    color=alt.Color('Significance_Color', \n",
    "                    scale=alt.Scale(domain=['Significant (P < 0.05)', 'Not Significant (P ‚â• 0.05)'], \n",
    "                                    range=['#2ca02c', '#ff7f0e']), \n",
    "                    title='Statistical Significance')\n",
    ")\n",
    "\n",
    "# Reference Line at OR = 1.0\n",
    "reference_line = alt.Chart(pd.DataFrame({'OR_1': [1]})).mark_rule(color='black', strokeDash=[3, 3]).encode(\n",
    "    x='OR_1:Q'\n",
    ")\n",
    "\n",
    "# Combine the bars and the reference line\n",
    "chart_odds_ratio = (reference_line + bars).properties(width=600)\n",
    "\n",
    "chart_odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0cee47",
   "metadata": {},
   "source": [
    "**Insights and Key Findings**\n",
    "\n",
    "> Finding: The difference between top performers and others is overwhelmingly defined by a future-oriented, growth mindset, not just raw intelligence. The effect of GTQ and most personality profiles completely disappears once behavioral strengths are accounted for.\n",
    "\n",
    "> Top 2 Non-Redundant Behavioral Drivers (The Core of R5)\n",
    "1. Futuristic (The Visionary Accelerator). This is the single strongest independent predictor in the entire model. It proves top performance is driven by a mindset focused on long-term strategy and conceiving future possibilities. Actionable Insight: Prioritize employees who naturally look ahead, conceptualize future possibilities, and drive long-term strategic value. This theme must be a core focus for hiring and promotion into innovation and leadership roles.\n",
    "\n",
    "2. Learner (The Continuous Improver). This relentless drive for skill mastery and knowledge acquisition is the second core ingredient for sustained top performance. Actionable Insight: We must recruit and coach individuals who see their work as an ongoing challenge to learn and improve. Incentivize skill acquisition to ensure employees remain relevant and valuable over time.\n",
    "\n",
    "> Conclusion: What Matters: The most successful employees are proactive, future-focused individuals who prioritize continuous learning Futuristic and Learner. What Doesn't Matter: The effect of raw intelligence GTQ and most personality types disappears when these behavioral themes are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598be8f4",
   "metadata": {},
   "source": [
    "## Contextual Factors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7de4321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_master from merge Structural Context tables (Grade, Position, Department, etc.)\n",
    "df_master = df_performance.merge(df_employees, on='employee_id', how='left')\n",
    "df_master = df_master.merge(df_grades, on='grade_id', how='left', suffixes=('', '_grade'))\n",
    "df_master = df_master.merge(df_positions, on='position_id', how='left', suffixes=('', '_position'))\n",
    "df_master = df_master.merge(df_departments, on='department_id', how='left', suffixes=('', '_dept'))\n",
    "df_master = df_master.merge(df_education, on='education_id', how='left', suffixes=('', '_education'))\n",
    "\n",
    "# Data Cleaning: Ensure 'rating' is numeric and valid\n",
    "df_master['rating'] = pd.to_numeric(df_master['rating'], errors='coerce')\n",
    "\n",
    "# Filter hanya rating yang valid (1.0 hingga 5.0)\n",
    "VALID_RATINGS = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "if 'name' in df_master.columns:\n",
    "    df_master = df_master.rename(columns={'name': 'grade_name'})\n",
    "\n",
    "# Membuat DataFrame sementara yang sudah bersih\n",
    "df_clean = df_master[df_master['rating'].isin(VALID_RATINGS)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d1fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis running on 2010 unique employees (using latest rating).\n",
      "Filtering data and preparing for model...\n",
      "Fitting Contextual Factors Logistic Regression Model...\n",
      "Fitting Complete.\n",
      "\n",
      "--- üèÜ Predictive Contextual Factors Ranking (Odds Ratio) ---\n",
      "Interpretation: OR > 1.0 increases odds of R5, OR < 1.0 decreases odds.\n",
      "                 Metric  Odds_Ratio  P_Value Significant         Significance_Color\n",
      "      name_education_S2      1.9730   0.0033           ‚úÖ     Significant (P < 0.05)\n",
      "     name_education_SMA      1.9326   0.0056           ‚úÖ     Significant (P < 0.05)\n",
      "      name_education_S1      1.5882   0.0555           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "          grade_name_IV      1.3079   0.1522           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "           grade_name_V      1.0654   0.7455           ‚ùå Not Significant (P ‚â• 0.05)\n",
      "years_of_service_months      1.0029   0.3387           ‚ùå Not Significant (P ‚â• 0.05)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2b9147a7580f40449c51100d2ffc11e2.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2b9147a7580f40449c51100d2ffc11e2.vega-embed details,\n",
       "  #altair-viz-2b9147a7580f40449c51100d2ffc11e2.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2b9147a7580f40449c51100d2ffc11e2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2b9147a7580f40449c51100d2ffc11e2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2b9147a7580f40449c51100d2ffc11e2\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"data\": {\"name\": \"data-361ea6f3ceae4bc75d7d8c51b7055154\"}, \"mark\": {\"type\": \"rule\", \"color\": \"black\", \"strokeDash\": [3, 3]}, \"encoding\": {\"x\": {\"field\": \"OR_1\", \"type\": \"quantitative\"}}}, {\"data\": {\"name\": \"data-9b39f1cab0ed0c0a776b564959c223b1\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Significance_Color\", \"scale\": {\"domain\": [\"Significant (P < 0.05)\", \"Not Significant (P \\u2265 0.05)\"], \"range\": [\"#2ca02c\", \"#ff7f0e\"]}, \"title\": \"Statistical Significance\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Metric\", \"type\": \"nominal\"}, {\"field\": \"Odds_Ratio\", \"format\": \".3f\", \"type\": \"quantitative\"}, {\"field\": \"P_Value\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"Significant\", \"type\": \"nominal\"}], \"x\": {\"field\": \"Odds_Ratio\", \"title\": \"Odds Ratio (e^Coefficient)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Metric\", \"sort\": {\"field\": \"Odds_Ratio\", \"op\": \"mean\", \"order\": \"descending\"}, \"title\": \"Contextual Factor\", \"type\": \"nominal\"}}, \"name\": \"view_5\", \"title\": \"Contextual Factors: Odds Ratio for Rating 5\"}], \"params\": [{\"name\": \"param_5\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_5\"]}], \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-361ea6f3ceae4bc75d7d8c51b7055154\": [{\"OR_1\": 1}], \"data-9b39f1cab0ed0c0a776b564959c223b1\": [{\"Metric\": \"name_education_S2\", \"Odds_Ratio\": 1.973, \"P_Value\": 0.0033, \"Significant\": \"\\u2705\", \"Significance_Color\": \"Significant (P < 0.05)\"}, {\"Metric\": \"name_education_SMA\", \"Odds_Ratio\": 1.9326, \"P_Value\": 0.0056, \"Significant\": \"\\u2705\", \"Significance_Color\": \"Significant (P < 0.05)\"}, {\"Metric\": \"name_education_S1\", \"Odds_Ratio\": 1.5882, \"P_Value\": 0.0555, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"grade_name_IV\", \"Odds_Ratio\": 1.3079, \"P_Value\": 0.1522, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"grade_name_V\", \"Odds_Ratio\": 1.0654, \"P_Value\": 0.7455, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}, {\"Metric\": \"years_of_service_months\", \"Odds_Ratio\": 1.0029, \"P_Value\": 0.3387, \"Significant\": \"\\u274c\", \"Significance_Color\": \"Not Significant (P \\u2265 0.05)\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CONFIGURATION: Define Specific Predictors (Based on your input) ---\n",
    "NUMERIC_CONTEXT_PREDICTORS = ['years_of_service_months']\n",
    "CATEGORICAL_CONTEXT_PREDICTORS = ['grade_name', 'name_education'] \n",
    "\n",
    "# --- 1. DATA FIX: Filter to ONE ROW PER EMPLOYEE (LATEST RATING) ---\n",
    "\n",
    "# We use df_clean, which contains historical data, and filter it down to the latest record per employee.\n",
    "df_context = (\n",
    "    df_clean\n",
    "    .sort_values(['employee_id', 'year'], ascending=[True, False])\n",
    "    .drop_duplicates('employee_id', keep='first')\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# 1B. Create the Binary Target Variable\n",
    "TARGET_VARIABLE = 'is_R5'\n",
    "df_context[TARGET_VARIABLE] = np.where(df_context['rating'] == 5.0, 1, 0)\n",
    "\n",
    "# 1C. Prepare Categorical Factors for Model\n",
    "df_dummies = pd.get_dummies(\n",
    "    df_context[CATEGORICAL_CONTEXT_PREDICTORS], \n",
    "    drop_first=True, \n",
    "    prefix=CATEGORICAL_CONTEXT_PREDICTORS\n",
    ")\n",
    "\n",
    "# Merge dummies back into the context DataFrame\n",
    "df_model_context = pd.concat([df_context, df_dummies], axis=1)\n",
    "\n",
    "# Compile the final list of dummy columns\n",
    "DUMMY_COLUMNS = list(df_dummies.columns)\n",
    "FINAL_PREDICTORS = NUMERIC_CONTEXT_PREDICTORS + DUMMY_COLUMNS\n",
    "\n",
    "print(f\"Analysis running on {len(df_context)} unique employees (using latest rating).\")\n",
    "\n",
    "# --- 2. Logistic Regression (With Value Error Fix) ---\n",
    "\n",
    "print(\"Filtering data and preparing for model...\")\n",
    "# Drop rows with any missing data across the target and all required predictors\n",
    "df_final_model = df_model_context[[TARGET_VARIABLE] + FINAL_PREDICTORS].dropna()\n",
    "\n",
    "# --- VALUE ERROR FIX: Ensure all predictors and target are numeric ---\n",
    "for col in NUMERIC_CONTEXT_PREDICTORS:\n",
    "    # Coerce non-numeric values in the numeric column to NaN (which are already dropped)\n",
    "    df_final_model[col] = pd.to_numeric(df_final_model[col], errors='coerce') \n",
    "\n",
    "Y = df_final_model[TARGET_VARIABLE].astype(int) # Target must be integer\n",
    "X = df_final_model[FINAL_PREDICTORS].astype(float) # Predictors must be float\n",
    "\n",
    "# Add a constant term (intercept)\n",
    "X = sm.add_constant(X, prepend=False)\n",
    "\n",
    "# Run the Model\n",
    "print(\"Fitting Contextual Factors Logistic Regression Model...\")\n",
    "model = sm.Logit(Y, X)\n",
    "result = model.fit(disp=False)\n",
    "print(\"Fitting Complete.\")\n",
    "\n",
    "# --- 3. Extract and Visualize Results ---\n",
    "\n",
    "# Calculate Odds Ratios and P-Values\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': result.params.index,\n",
    "    'Odds_Ratio': np.exp(result.params.values),\n",
    "    'P_Value': result.pvalues.values\n",
    "})\n",
    "\n",
    "# Filter out the constant term and determine significance\n",
    "results_df = results_df[results_df['Metric'] != 'const']\n",
    "results_df['Significant'] = np.where(results_df['P_Value'] < 0.05, '‚úÖ', '‚ùå')\n",
    "results_df['Significance_Color'] = np.where(results_df['P_Value'] < 0.05, 'Significant (P < 0.05)', 'Not Significant (P ‚â• 0.05)')\n",
    "\n",
    "df_final_ranking = results_df.sort_values(by='Odds_Ratio', ascending=False).round(4)\n",
    "\n",
    "print(\"\\n--- üèÜ Predictive Contextual Factors Ranking (Odds Ratio) ---\")\n",
    "print(\"Interpretation: OR > 1.0 increases odds of R5, OR < 1.0 decreases odds.\")\n",
    "print(df_final_ranking.to_string(index=False))\n",
    "\n",
    "\n",
    "# --- 4. Visualization (Odds Ratio Forest Plot) ---\n",
    "\n",
    "# Set a baseline for the Y-axis (OR = 1.0)\n",
    "base = alt.Chart(df_final_ranking).encode(\n",
    "    y=alt.Y('Metric', sort=alt.EncodingSortField(field='Odds_Ratio', op='mean', order='descending'), title='Contextual Factor'),\n",
    "    tooltip=['Metric', alt.Tooltip('Odds_Ratio', format='.3f'), alt.Tooltip('P_Value', format='.4f'), 'Significant']\n",
    ").properties(\n",
    "    title='Contextual Factors: Odds Ratio for Rating 5'\n",
    ").interactive()\n",
    "\n",
    "# Bar Chart for Magnitude (Odds Ratio)\n",
    "bars = base.mark_bar().encode(\n",
    "    x=alt.X('Odds_Ratio', title='Odds Ratio (e^Coefficient)'),\n",
    "    color=alt.Color('Significance_Color', \n",
    "                    scale=alt.Scale(domain=['Significant (P < 0.05)', 'Not Significant (P ‚â• 0.05)'], \n",
    "                                    range=['#2ca02c', '#ff7f0e']), # Green/Orange color scheme\n",
    "                    title='Statistical Significance')\n",
    ")\n",
    "\n",
    "# Reference Line at OR = 1.0\n",
    "reference_line = alt.Chart(pd.DataFrame({'OR_1': [1]})).mark_rule(color='black', strokeDash=[3, 3]).encode(\n",
    "    x='OR_1:Q'\n",
    ")\n",
    "\n",
    "# Combine the bars and the reference line\n",
    "chart_odds_ratio = (reference_line + bars).properties(width=600)\n",
    "\n",
    "chart_odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f2bbe",
   "metadata": {},
   "source": [
    "**Insight and Key Findings**\n",
    "\n",
    "> Finding: The only factors that are statistically reliable drivers of top performance are specific levels of Educational Attainment. Grade and Tenure do not independently predict who achieves a Rating 5.\n",
    "\n",
    "> Top Drivers of Rating 5\n",
    "1. Education S2 (The Master's Advantage). Having a Master's degree gives an employee a significantly higher chance of achieving a Rating 5 compared to the baseline education level. This is the strongest structural predictor you have.Action: Prioritize the hiring and attraction of talent with postgraduate degrees. These candidates represent the highest probability of achieving top performance.\n",
    "\n",
    "2. Education SMA (The High School Excellence). Having a High School degree (SMA) also gives an employee a much higher chance of achieving a Rating 5 compared to the baseline. This reveals a surprisingly high-potential talent pool. Action: Investigate this high-performing SMA group further (e.g., check their natural ability or strengths). They may offer an excellent talent source that excels due to strong inherent talent despite having less formal schooling.\n",
    "\n",
    "> Conclusion: Education is the dominant structural factor. We must stop relying on traditional metrics like tenure and internal grade movement to predict success, as the data proves they are non-factors. Instead, the strategy must prioritize the proven high-potential groups defined by the S2 and SMA education levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294dea34",
   "metadata": {},
   "source": [
    "# The Rating 5 Success Formula: Rule-Based Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04017a30",
   "metadata": {},
   "source": [
    "## **The Most Critical Findings**\n",
    "\n",
    "The following points synthesize the most powerful and reliable insights from our **Competency, Psychometric, Behavioral, and Contextual Factor** analyses, demonstrating how performance emerges from multiple factors.\n",
    "\n",
    "> 1. The Core Behavioral Mindset is Non-Negotiable (The Engine)\n",
    "\n",
    "This finding comes from the **Behavioral Strengths Analysis**, which tested all factors simultaneously.\n",
    "\n",
    "* **The Finding:** When intelligence ($\\text{GTQ}$), competencies, and all other factors are controlled for, the **Futuristic** and **Learner** themes are the **only factors** that remain statistically significant and independently accelerate performance.\n",
    "* **The Takeaway:** **Mindset is the primary driver.** No amount of competence or experience can compensate for a lack of a future-focused, growth-oriented mindset. This must be the foundation of your talent strategy.\n",
    "* **Formula Justification:** This is why the **Behavioral Mindset** receives the majority weight ($\\mathbf{75\\%}$ in the Success Formula)‚Äîit is the **true, non-redundant engine** of $\\text{R5}$ performance.\n",
    "\n",
    "---\n",
    "\n",
    "> 2. Execution Competency Defines the Output (The Transmission)\n",
    "\n",
    "This finding comes from the **Competency Pillars Analysis** ($\\text{R5}$ vs. $\\text{R1/R2}$).\n",
    "\n",
    "* **The Finding:** The largest performance gaps were found in the $\\text{Value Creation for Users}$ and $\\text{Insight \\& Decision Sharpness}$ competencies.\n",
    "* **The Takeaway:** $\\text{R5}$ is defined by **impact, not just effort**. The top performers excel at translating their mindset into measurable value and clear, correct decisions.\n",
    "* **Formula Justification:** The $\\mathbf{15\\%}$ weight assigned to **Execution Competency** ensures the formula requires employees to have the necessary skills to deliver the superior outcomes demanded by their $\\text{R5}$ Mindset.\n",
    "\n",
    "---\n",
    "\n",
    "> 3. The Structural Filters Are Surprising and Narrow (The Chassis)\n",
    "\n",
    "This finding comes from the **Contextual Factor Analysis** (testing Grade, Tenure, Education).\n",
    "\n",
    "* **The Finding:** The only factors that act as significant structural filters are specific **Education Levels ($\\text{S2}$ and $\\text{SMA}$)**. Factors like **Years of Service (Tenure) and Grade Level are proven to be irrelevant** as independent predictors.\n",
    "* **The Takeaway:** **Tenure and Grade are misleading metrics.** You should immediately stop relying on them for talent prediction. Strategy should focus on two distinct, high-potential talent pools defined by their education.\n",
    "* **Formula Justification:** This insight allows us to **eliminate non-factors** and assign the smallest, but still necessary, weight ($\\mathbf{10\\%}$) to the few structural requirements that actually matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6c373",
   "metadata": {},
   "source": [
    "## **The Success Formula Structure**\n",
    "The formula establishes the relative importance of talent factors based on their statistical power to drive Rating 5. \n",
    "\n",
    "$$\\mathbf{\\text{Rating 5 Potential}} = \\mathbf{75\\% \\text{ Behavioral Mindset}} + \\mathbf{15\\% \\text{ Execution Competency}} + \\mathbf{10\\% \\text{ Structural Filter}}$$\n",
    "\n",
    "\n",
    "**Justification for Weights (Why These Percentages?)**\n",
    "\n",
    "The weights assigned are a management decision directly reflecting the statistical hierarchy proven by the data:\n",
    "* 75% Behavioral Mindset: This received the highest weight because the Futuristic and Learner themes were the only statistically significant, non-redundant accelerators in the final, integrated analysis. They are the core engine of sustained $\\text{R5}$ performance.\n",
    "* 15% Execution Competency: Represents essential execution skills (e.g., $\\text{Value Creation}$) which showed the largest performance gaps in the initial competency analysis. These skills are necessary to translate the mindset into tangible results.\n",
    "* 10% Structural Filter: Represents the foundational requirement of the $\\text{S2}$ and $\\text{SMA}$ education levels, which were the only significant structural predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
